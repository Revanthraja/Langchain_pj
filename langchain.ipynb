{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4410828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"your_api_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d970183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525a8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fad3d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "tex=\"what are the 4 vaccation destinations for the someone who like Technology\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "287778cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=\"what is the diffrence between llm and gpt index \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b93dda7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LLM (Logical Log Manager) is a database indexing system that uses a logical log to store and manage data. It is designed to provide fast access to data stored in a database. LLM is used to improve the performance of queries and transactions.\n",
      "\n",
      "GPT (Global Partition Table) is a database indexing system that uses a global partition table to store and manage data. It is designed to provide fast access to data stored in a database. GPT is used to improve the scalability of queries and transactions.\n"
     ]
    }
   ],
   "source": [
    "print(llm(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "70928022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Tokyo, Japan\n",
      "2. San Francisco, California\n",
      "3. Seoul, South Korea\n",
      "4. Singapore\n"
     ]
    }
   ],
   "source": [
    "print(llm(tex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6274d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5dd99cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "\n",
    "input_variables=['food'],\n",
    "    template=\"what are your favorite {food}\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "26bfbbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=LLMChain(llm=llm,prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de514a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "My favorite countries are Italy, France, and Japan.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run('country'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff169e78",
   "metadata": {},
   "source": [
    "# Training a our own custom data using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99cb9492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d7b7b",
   "metadata": {},
   "source": [
    "Importing the requried libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1416d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma,Pinecone\n",
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "from langchain import OpenAI,VectorDBQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d9188e",
   "metadata": {},
   "source": [
    "# Loading the Text documents from the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "034cd521",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=DirectoryLoader('/media/revanth/Data/NLP_R/new',glob='**/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6c48784",
   "metadata": {},
   "outputs": [],
   "source": [
    "document=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97b02730",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_splitter=CharacterTextSplitter(chunk_size=100,chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c89b4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 101, which is longer than the specified 100\n",
      "Created a chunk of size 103, which is longer than the specified 100\n",
      "Created a chunk of size 102, which is longer than the specified 100\n",
      "Created a chunk of size 104, which is longer than the specified 100\n",
      "Created a chunk of size 101, which is longer than the specified 100\n",
      "Created a chunk of size 121, which is longer than the specified 100\n",
      "Created a chunk of size 107, which is longer than the specified 100\n",
      "Created a chunk of size 129, which is longer than the specified 100\n",
      "Created a chunk of size 138, which is longer than the specified 100\n",
      "Created a chunk of size 140, which is longer than the specified 100\n",
      "Created a chunk of size 132, which is longer than the specified 100\n",
      "Created a chunk of size 159, which is longer than the specified 100\n",
      "Created a chunk of size 127, which is longer than the specified 100\n",
      "Created a chunk of size 173, which is longer than the specified 100\n",
      "Created a chunk of size 162, which is longer than the specified 100\n",
      "Created a chunk of size 110, which is longer than the specified 100\n",
      "Created a chunk of size 105, which is longer than the specified 100\n",
      "Created a chunk of size 202, which is longer than the specified 100\n",
      "Created a chunk of size 137, which is longer than the specified 100\n",
      "Created a chunk of size 135, which is longer than the specified 100\n",
      "Created a chunk of size 107, which is longer than the specified 100\n",
      "Created a chunk of size 223, which is longer than the specified 100\n",
      "Created a chunk of size 169, which is longer than the specified 100\n",
      "Created a chunk of size 160, which is longer than the specified 100\n",
      "Created a chunk of size 151, which is longer than the specified 100\n",
      "Created a chunk of size 140, which is longer than the specified 100\n",
      "Created a chunk of size 108, which is longer than the specified 100\n",
      "Created a chunk of size 148, which is longer than the specified 100\n",
      "Created a chunk of size 175, which is longer than the specified 100\n",
      "Created a chunk of size 145, which is longer than the specified 100\n",
      "Created a chunk of size 278, which is longer than the specified 100\n",
      "Created a chunk of size 216, which is longer than the specified 100\n",
      "Created a chunk of size 160, which is longer than the specified 100\n",
      "Created a chunk of size 216, which is longer than the specified 100\n",
      "Created a chunk of size 274, which is longer than the specified 100\n",
      "Created a chunk of size 200, which is longer than the specified 100\n",
      "Created a chunk of size 199, which is longer than the specified 100\n",
      "Created a chunk of size 138, which is longer than the specified 100\n",
      "Created a chunk of size 232, which is longer than the specified 100\n",
      "Created a chunk of size 176, which is longer than the specified 100\n",
      "Created a chunk of size 241, which is longer than the specified 100\n",
      "Created a chunk of size 292, which is longer than the specified 100\n",
      "Created a chunk of size 149, which is longer than the specified 100\n",
      "Created a chunk of size 179, which is longer than the specified 100\n",
      "Created a chunk of size 188, which is longer than the specified 100\n",
      "Created a chunk of size 314, which is longer than the specified 100\n",
      "Created a chunk of size 167, which is longer than the specified 100\n",
      "Created a chunk of size 168, which is longer than the specified 100\n",
      "Created a chunk of size 324, which is longer than the specified 100\n",
      "Created a chunk of size 283, which is longer than the specified 100\n",
      "Created a chunk of size 140, which is longer than the specified 100\n",
      "Created a chunk of size 220, which is longer than the specified 100\n",
      "Created a chunk of size 200, which is longer than the specified 100\n",
      "Created a chunk of size 315, which is longer than the specified 100\n",
      "Created a chunk of size 187, which is longer than the specified 100\n",
      "Created a chunk of size 263, which is longer than the specified 100\n",
      "Created a chunk of size 138, which is longer than the specified 100\n",
      "Created a chunk of size 137, which is longer than the specified 100\n",
      "Created a chunk of size 135, which is longer than the specified 100\n",
      "Created a chunk of size 110, which is longer than the specified 100\n",
      "Created a chunk of size 262, which is longer than the specified 100\n",
      "Created a chunk of size 267, which is longer than the specified 100\n",
      "Created a chunk of size 255, which is longer than the specified 100\n",
      "Created a chunk of size 201, which is longer than the specified 100\n",
      "Created a chunk of size 124, which is longer than the specified 100\n",
      "Created a chunk of size 240, which is longer than the specified 100\n",
      "Created a chunk of size 113, which is longer than the specified 100\n",
      "Created a chunk of size 366, which is longer than the specified 100\n",
      "Created a chunk of size 186, which is longer than the specified 100\n",
      "Created a chunk of size 113, which is longer than the specified 100\n",
      "Created a chunk of size 329, which is longer than the specified 100\n",
      "Created a chunk of size 343, which is longer than the specified 100\n",
      "Created a chunk of size 134, which is longer than the specified 100\n",
      "Created a chunk of size 199, which is longer than the specified 100\n",
      "Created a chunk of size 129, which is longer than the specified 100\n",
      "Created a chunk of size 114, which is longer than the specified 100\n",
      "Created a chunk of size 222, which is longer than the specified 100\n",
      "Created a chunk of size 174, which is longer than the specified 100\n",
      "Created a chunk of size 107, which is longer than the specified 100\n",
      "Created a chunk of size 104, which is longer than the specified 100\n",
      "Created a chunk of size 109, which is longer than the specified 100\n",
      "Created a chunk of size 159, which is longer than the specified 100\n",
      "Created a chunk of size 139, which is longer than the specified 100\n",
      "Created a chunk of size 223, which is longer than the specified 100\n",
      "Created a chunk of size 161, which is longer than the specified 100\n",
      "Created a chunk of size 138, which is longer than the specified 100\n",
      "Created a chunk of size 143, which is longer than the specified 100\n",
      "Created a chunk of size 342, which is longer than the specified 100\n",
      "Created a chunk of size 156, which is longer than the specified 100\n",
      "Created a chunk of size 108, which is longer than the specified 100\n",
      "Created a chunk of size 233, which is longer than the specified 100\n",
      "Created a chunk of size 327, which is longer than the specified 100\n",
      "Created a chunk of size 122, which is longer than the specified 100\n",
      "Created a chunk of size 178, which is longer than the specified 100\n",
      "Created a chunk of size 132, which is longer than the specified 100\n",
      "Created a chunk of size 131, which is longer than the specified 100\n",
      "Created a chunk of size 121, which is longer than the specified 100\n",
      "Created a chunk of size 145, which is longer than the specified 100\n",
      "Created a chunk of size 124, which is longer than the specified 100\n",
      "Created a chunk of size 163, which is longer than the specified 100\n",
      "Created a chunk of size 131, which is longer than the specified 100\n",
      "Created a chunk of size 112, which is longer than the specified 100\n",
      "Created a chunk of size 105, which is longer than the specified 100\n",
      "Created a chunk of size 118, which is longer than the specified 100\n",
      "Created a chunk of size 106, which is longer than the specified 100\n",
      "Created a chunk of size 166, which is longer than the specified 100\n",
      "Created a chunk of size 111, which is longer than the specified 100\n",
      "Created a chunk of size 110, which is longer than the specified 100\n",
      "Created a chunk of size 101, which is longer than the specified 100\n",
      "Created a chunk of size 114, which is longer than the specified 100\n",
      "Created a chunk of size 103, which is longer than the specified 100\n",
      "Created a chunk of size 204, which is longer than the specified 100\n",
      "Created a chunk of size 212, which is longer than the specified 100\n",
      "Created a chunk of size 232, which is longer than the specified 100\n",
      "Created a chunk of size 212, which is longer than the specified 100\n",
      "Created a chunk of size 412, which is longer than the specified 100\n",
      "Created a chunk of size 356, which is longer than the specified 100\n",
      "Created a chunk of size 338, which is longer than the specified 100\n",
      "Created a chunk of size 367, which is longer than the specified 100\n",
      "Created a chunk of size 458, which is longer than the specified 100\n",
      "Created a chunk of size 410, which is longer than the specified 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 219, which is longer than the specified 100\n",
      "Created a chunk of size 231, which is longer than the specified 100\n",
      "Created a chunk of size 389, which is longer than the specified 100\n",
      "Created a chunk of size 346, which is longer than the specified 100\n",
      "Created a chunk of size 160, which is longer than the specified 100\n",
      "Created a chunk of size 137, which is longer than the specified 100\n",
      "Created a chunk of size 120, which is longer than the specified 100\n",
      "Created a chunk of size 110, which is longer than the specified 100\n",
      "Created a chunk of size 746, which is longer than the specified 100\n",
      "Created a chunk of size 687, which is longer than the specified 100\n",
      "Created a chunk of size 843, which is longer than the specified 100\n",
      "Created a chunk of size 624, which is longer than the specified 100\n",
      "Created a chunk of size 713, which is longer than the specified 100\n",
      "Created a chunk of size 910, which is longer than the specified 100\n",
      "Created a chunk of size 551, which is longer than the specified 100\n",
      "Created a chunk of size 648, which is longer than the specified 100\n",
      "Created a chunk of size 462, which is longer than the specified 100\n",
      "Created a chunk of size 215, which is longer than the specified 100\n",
      "Created a chunk of size 367, which is longer than the specified 100\n",
      "Created a chunk of size 232, which is longer than the specified 100\n",
      "Created a chunk of size 190, which is longer than the specified 100\n",
      "Created a chunk of size 250, which is longer than the specified 100\n",
      "Created a chunk of size 109, which is longer than the specified 100\n",
      "Created a chunk of size 299, which is longer than the specified 100\n",
      "Created a chunk of size 134, which is longer than the specified 100\n",
      "Created a chunk of size 399, which is longer than the specified 100\n",
      "Created a chunk of size 687, which is longer than the specified 100\n",
      "Created a chunk of size 332, which is longer than the specified 100\n",
      "Created a chunk of size 176, which is longer than the specified 100\n",
      "Created a chunk of size 177, which is longer than the specified 100\n",
      "Created a chunk of size 633, which is longer than the specified 100\n",
      "Created a chunk of size 423, which is longer than the specified 100\n",
      "Created a chunk of size 167, which is longer than the specified 100\n",
      "Created a chunk of size 450, which is longer than the specified 100\n",
      "Created a chunk of size 374, which is longer than the specified 100\n"
     ]
    }
   ],
   "source": [
    "texts=txt_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf7495a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd0fc89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding=OpenAIEmbeddings(openai_api_key=os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089e2805",
   "metadata": {},
   "source": [
    "Chroma makes it easy to build LLM apps by making knowledge, facts, and skills pluggable for LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a7794f",
   "metadata": {},
   "source": [
    "# Training the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1e4abed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n"
     ]
    }
   ],
   "source": [
    "docsearch=Chroma.from_documents(texts,embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68ff9735",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=VectorDBQA.from_chain_type(llm=OpenAI(),chain_type='stuff',vectorstore=docsearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a0ec3",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f4e00f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Revanthraja M is a student of SJCIT.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"who is Revanthraja\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6f04e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Dr. Bharthi M is a professor in the Dept. of CSE at holiness Jagadguru Sri Sri Sri Dr. Nirmalanandanatha Swamiji of Sri Adichunchanagiri Mutt.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"Who is Bharthi \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "893c1b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Dr. Ravi Kumar K M'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"Who is the principle of SJCIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bdcee53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Automation in cars can refer to the use of technology to help drivers be aware of their surroundings and make driving safer. This can include features such as lane departure warnings, automatic emergency braking, and blind spot monitoring.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"Automation in CAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "780938ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Automation in cars to alert drivers is a system design that uses technology to alert drivers of potential hazards on the road. It could include features such as automated braking, lane departure warning, and blind spot detection.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run('Automation in car to alert drivers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b2a3529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Dr. T Munikenche Gowda.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run('who is the director of BGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc603b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' There are two chapters in the text.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"Chapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1953a55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A chatbot (also known as a talkbot, chatterbot, Bot, IM bot, interactive agent, or Artificial Conversational Entity) is a computer program or an artificial intelligence which conducts a conversation via auditory or textual methods.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"Chatterbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbb45959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I am on the Internet.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"where is your location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9dee9d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I don't have a phone number configured.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run('what is your phone number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a99d884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" That's my name.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run('what is your name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "115bc788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' My interests include robotics, computer science, natural language processing, and reading.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run('what are your intersets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5a1fdc",
   "metadata": {},
   "source": [
    "# YOUTUBE TRANSCRIPTS API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c603e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import YoutubeLoader,UnstructuredPDFLoader,OnlinePDFLoader\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "37772f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=YoutubeLoader.from_youtube_url(\"https://www.youtube.com/watch?v=pNcQ5XXMgH4&list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5&index=7\",add_video_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41889b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfb9ff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c74ccbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found video from Data Independent that is 668 seconds_long\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found video from {result[0].metadata['author']} that is {result[0].metadata['length']} seconds_long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b48fbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This tutorial explains how to use the Lang Chain library to take YouTube transcripts and pass them to Open AI to generate a summary. It also explains how to use the recursive character splitter to split up long transcripts into smaller chunks, and how to use the mapreduce method to generate a summary of multiple videos. Finally, it explains how to use the summarize scan to generate a summary of multiple videos.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=load_summarize_chain(llm,chain_type='stuff',verbose=False)\n",
    "chain.run(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6117a0b2",
   "metadata": {},
   "source": [
    "#  Ask a BOOK Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e85a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadr=UnstructuredPDFLoader('FM.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a539f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=loadr.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43c5a9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you have 1 document(s) in your data\n",
      " There are 313570 characters in your document\n"
     ]
    }
   ],
   "source": [
    "print(f\" you have {len(data1)} document(s) in your data\")\n",
    "print(f\" There are {len(data1[0].page_content)} characters in your document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3167b5b5",
   "metadata": {},
   "source": [
    "# Chunk your data up into smaller documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7754d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=0)\n",
    "text=text_splitter.split_documents(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35eea0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 387 documents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Now you have {len(text)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef0343a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Feature Engineering Machine Learning AND TECHNIQUES FOR DATA SCIENTISTS\\n\\nFeature Engineering for Machine Learning PRINCIPLES AND TECHNIQUES FOR DATA SCIENTISTS\\n\\nAlice Zheng & Amanda Casari\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFeature Engineering for Machine Learning\\n\\nPrinciples and Techniques for Data Scientists\\n\\nAlice Zheng and Amanda Casari\\n\\nBoston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing\\n\\nFeature Engineering for Machine Learning\\n\\nCopyright © 2018 Alice Zheng, Amanda Casari. All rights reserved.\\n\\nPrinted in the United States of America.\\n\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\n\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles ( http://oreilly.com/safari ). For more information, contact our corporate/insti‐ tutional sales department: 800-998-9938 or corporate@oreilly.com .', lookup_str='', metadata={'source': 'FM.pdf'}, lookup_index=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3acdd28",
   "metadata": {},
   "source": [
    "# Create embeddings of your documets to get ready for semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8c2695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddig=OpenAIEmbeddings(openai_api_key=os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b740ffe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n"
     ]
    }
   ],
   "source": [
    "docsearch=Chroma.from_documents(text,embeddig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2bfc7801",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=VectorDBQA.from_chain_type(llm=OpenAI(),chain_type='stuff',vectorstore=docsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8211e4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Tasks that involve processing and understanding visual and auditory signals require different approaches than language tasks, which are learned and require intentional training.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"Tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19312a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A pipeline is a multistage, iterative process in which data is observed, aggregated, stored, bought, converted, pulled, subsampled, massaged, cleaned, dumped, converted, modeled, evaluated, iterated, rewritten, and run before final predictions are outputted.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"what is pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0dd182e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Alice Zheng and Amanda Casari'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"who is the author\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74128209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Binning is a feature engineering technique used in machine learning. It involves dividing data into bins of fixed or adaptive widths to help create a clearer picture of the data.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"what is Binning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ae77970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Scalar is a single numeric feature and a vector is an ordered list of scalars.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"What is the scalers and vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "01de1663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Feature selection techniques prune away nonuseful features in order to reduce the complexity of the resulting model. The end goal is a parsimonious model that is quicker to compute, with little or no degradation in predictive accuracy. In order to arrive at such a model, some feature selection techniques require training more than one candidate model. In other words, feature selection is not about reducing training time—in fact, some techniques increase overall training time—but about reducing model scoring time. Roughly speaking, feature selection techniques fall into three classes: filtering techniques, wrapper methods, and embedded methods.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"What is Feature selections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "154d4e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A good data science team includes people with a variety of skills such as statisticians, computer scientists, software engineers, domain experts, and machine learning engineers.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"What are examples of good data science teams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e45d18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Data are observations of real-world phenomena. Examples of data include stock market data, personal biometric data, and customer intelligence data.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"what is Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "96e18bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nFeature engineering is the act of extracting features from raw data and transforming them into formats that are suitable for the machine learning model. It is a crucial step in the machine learning pipeline, because the right features can ease the difficulty of modeling, and therefore enable the pipeline to output results of higher quality.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"what is Feature engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79afb022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
